/**
 * Reactive AI chat state with multi-conversation support.
 *
 * Uses one TanStack AI `createChat()` instance per conversation, stored
 * in a Map. Each instance owns its own stream, messages, and lifecycle.
 * Switching conversations just changes which instance the getters read
 * from — no `setMessages()` swapping, no background stream tracking.
 *
 * Background streaming is free: when the user switches away from a
 * streaming conversation, its ChatClient keeps streaming. The completed
 * response appears in Y.Doc via that instance's `onFinish`. Multiple
 * conversations can stream concurrently.
 *
 * Provider and model are stored per-conversation in the `conversations`
 * table, surviving reloads and syncing across devices.
 *
 * @example
 * ```svelte
 * <script>
 *   import { aiChatState } from '$lib/state/chat.svelte';
 * </script>
 *
 * {#each aiChatState.conversations as conv (conv.id)}
 *   <button onclick={() => aiChatState.switchConversation(conv.id)}>
 *     {conv.title}
 *   </button>
 * {/each}
 *
 * {#each aiChatState.messages as message (message.id)}
 *   <ChatBubble {message} />
 * {/each}
 * ```
 */

import { generateId } from '@epicenter/hq';
import { ANTHROPIC_MODELS } from '@tanstack/ai-anthropic';
import { GeminiTextModels } from '@tanstack/ai-gemini';
import { GROK_CHAT_MODELS } from '@tanstack/ai-grok';
import { OPENAI_CHAT_MODELS } from '@tanstack/ai-openai';
import type { UIMessage } from '@tanstack/ai-svelte';
import { createChat, fetchServerSentEvents } from '@tanstack/ai-svelte';
import { getServerUrl } from '$lib/state/settings';
import type { ChatMessage, Conversation } from '$lib/workspace';
import { popupWorkspace } from '$lib/workspace-popup';

// ─────────────────────────────────────────────────────────────────────────────
// Provider / Model Configuration
// ─────────────────────────────────────────────────────────────────────────────

/**
 * Model arrays imported from TanStack AI provider packages.
 *
 * These are maintained by the TanStack AI team — no local hardcoded lists.
 * To update model lists, run: `bun update @tanstack/ai-openai @tanstack/ai-anthropic ...`
 *
 * Arrays are ordered newest-first by the upstream packages.
 */
const PROVIDER_MODELS = {
	openai: OPENAI_CHAT_MODELS,
	anthropic: ANTHROPIC_MODELS,
	gemini: GeminiTextModels,
	grok: GROK_CHAT_MODELS,
	// Ollama models are curated here (not imported from @tanstack/ai-ollama)
	// because that package pulls in the `ollama` SDK which depends on node:fs,
	// breaking the browser extension build. Users can type any model name
	// via the combobox's freeform input.
	ollama: [
		'deepseek-r1',
		'qwen3',
		'llama4',
		'gemma3',
		'phi4',
		'mistral',
		'codellama',
		'llama3',
	] as const,
} as const;

type Provider = keyof typeof PROVIDER_MODELS;

const DEFAULT_PROVIDER: Provider = 'anthropic';
const DEFAULT_MODEL = PROVIDER_MODELS[DEFAULT_PROVIDER][0];
const AVAILABLE_PROVIDERS = Object.keys(PROVIDER_MODELS) as Provider[];

// ─────────────────────────────────────────────────────────────────────────────
// Server URL Cache
// ─────────────────────────────────────────────────────────────────────────────

/**
 * Cached server URL for synchronous access.
 *
 * `fetchServerSentEvents` requires a synchronous URL getter (`string | (() => string)`).
 * We initialize with the default and update asynchronously from settings.
 * For 99% of users the default never changes.
 */
let serverUrlCache = 'http://127.0.0.1:3913';
void getServerUrl().then((url) => {
	serverUrlCache = url;
});

// ─────────────────────────────────────────────────────────────────────────────
// State Factory
// ─────────────────────────────────────────────────────────────────────────────

function createAiChatState() {
	// ── Conversation List (Y.Doc-backed) ──────────────────────────────

	/** Read all conversations sorted by most recently updated first. */
	const readAllConversations = (): Conversation[] =>
		popupWorkspace.tables.conversations
			.getAllValid()
			.sort((a, b) => b.updatedAt - a.updatedAt);

	let conversations = $state<Conversation[]>(readAllConversations());

	// Re-read on every Y.Doc change — observer fires on persistence load
	// and any subsequent remote/local modification.
	popupWorkspace.tables.conversations.observe(() => {
		conversations = readAllConversations();
	});

	// Refresh message list when messages are added/updated (e.g. background completion).
	// Uses changedIds to resolve which conversations are affected — only those
	// conversations get their messages refreshed instead of all cached instances.
	popupWorkspace.tables.chatMessages.observe((changedIds) => {
		const affectedConversations = new Set<string>();

		for (const msgId of changedIds) {
			const result = popupWorkspace.tables.chatMessages.get(msgId);
			if (result.status !== 'valid') continue;
			affectedConversations.add(result.row.conversationId);
		}

		for (const conversationId of affectedConversations) {
			const instance = chatInstances.get(conversationId);
			if (!instance) continue;
			// Skip streaming instances — they are already reactive to their own stream
			if (instance.isLoading) continue;
			instance.setMessages(loadMessagesForConversation(conversationId));
		}
	});
	// ── Active Conversation ───────────────────────────────────────────

	/** Initialize to the most recent conversation, or null if none exist. */
	let activeConversationId = $state<string | null>(
		conversations[0]?.id ?? null,
	);

	/**
	 * Derived from `activeConversationId` + `conversations`.
	 *
	 * Re-evaluates when either changes — e.g. when provider/model is
	 * updated in the table, the observer fires, `conversations` updates,
	 * and this re-derives with the new metadata.
	 */
	const activeConversation = $derived(
		conversations.find((c) => c.id === activeConversationId) ?? null,
	);

	// ── Helpers ───────────────────────────────────────────────────────

	/**
	 * Get the active conversation by reading `$state` directly.
	 *
	 * Used in non-reactive contexts (async callbacks, event handlers)
	 * where `$derived` tracking isn't needed.
	 */
	const getActiveConversation = (): Conversation | null =>
		conversations.find((c) => c.id === activeConversationId) ?? null;

	/** Load persisted messages for a conversation from Y.Doc. */
	const loadMessagesForConversation = (conversationId: string) =>
		popupWorkspace.tables.chatMessages
			.getAllValid()
			.filter((m) => m.conversationId === conversationId)
			.sort((a, b) => a.createdAt - b.createdAt)
			.map(toUiMessage);

	// ── Per-Conversation ChatClient Instances ─────────────────────────

	/**
	 * Map of conversation ID → ChatClient instance.
	 *
	 * Each conversation gets its own `createChat()` instance with baked-in
	 * `conversationId` in the connection callback and `onFinish`. This means:
	 * - Background streaming is free (each instance owns its stream)
	 * - `onFinish` always persists to the correct conversation
	 * - No `setMessages()` swapping needed on conversation switch
	 * - Multiple conversations can stream concurrently
	 */
	const chatInstances = new Map<string, ReturnType<typeof createChat>>();

	/**
	 * Maximum number of ChatClient instances to keep in memory.
	 *
	 * When exceeded, idle (non-active, non-streaming) instances are evicted
	 * oldest-first (Map iteration order is insertion order). Evicted
	 * conversations recreate their instance on next access via `ensureChat`.
	 */
	const MAX_CACHED_INSTANCES = 20;

	/**
	 * Evict idle ChatClient instances when the cache exceeds the limit.
	 *
	 * Preserves the active conversation's instance and any instance that
	 * is currently streaming. Iterates in insertion order (oldest first).
	 */
	function evictStaleInstances() {
		if (chatInstances.size <= MAX_CACHED_INSTANCES) return;

		for (const [id, instance] of chatInstances) {
			if (chatInstances.size <= MAX_CACHED_INSTANCES) break;
			if (id === activeConversationId) continue;
			if (instance.isLoading) continue;
			chatInstances.delete(id);
		}
	}

	/**
	 * Get or create a ChatClient for a conversation.
	 *
	 * Lazily creates instances on first access. The connection callback
	 * reads the conversation's provider/model at request time (not creation
	 * time) so provider/model changes take effect on the next send.
	 *
	 * Triggers eviction when the instance cache exceeds `MAX_CACHED_INSTANCES`.
	 *
	 * @example
	 * ```typescript
	 * const chat = ensureChat('conv-123');
	 * chat.sendMessage({ content: 'Hello' });
	 * ```
	 */
	function ensureChat(conversationId: string): ReturnType<typeof createChat> {
		const existing = chatInstances.get(conversationId);
		if (existing) return existing;

		const instance = createChat({
			initialMessages: loadMessagesForConversation(conversationId),
			connection: fetchServerSentEvents(
				() => `${serverUrlCache}/ai/chat`,
				async () => {
					// Read conversation at request time for fresh provider/model
					const conv = conversations.find((c) => c.id === conversationId);
					return {
						body: {
							provider: conv?.provider ?? DEFAULT_PROVIDER,
							model: conv?.model ?? DEFAULT_MODEL,
							conversationId,
							systemPrompt: conv?.systemPrompt ?? undefined,
						},
					};
				},
			),
			onFinish: (message) => {
				// conversationId is baked in — can never target the wrong conversation
				popupWorkspace.tables.chatMessages.set({
					id: message.id,
					conversationId,
					role: 'assistant',
					parts: message.parts,
					createdAt: message.createdAt?.getTime() ?? Date.now(),
					_v: 1,
				});

				// Touch conversation's updatedAt so it floats to top of list
				const conv = conversations.find((c) => c.id === conversationId);
				if (conv) {
					popupWorkspace.tables.conversations.set({
						...conv,
						updatedAt: Date.now(),
					});
				}
			},
		});

		chatInstances.set(conversationId, instance);
		evictStaleInstances();
		return instance;
	}

	// ── Conversation CRUD ─────────────────────────────────────────────

	/**
	 * Create a new conversation and switch to it.
	 *
	 * Inherits provider/model from the current conversation so new
	 * threads continue with the user's preferred settings.
	 *
	 * @returns The new conversation's ID.
	 */
	function createConversation(opts?: {
		title?: string;
		parentId?: string;
		sourceMessageId?: string;
		systemPrompt?: string;
	}): string {
		const id = generateId();
		const now = Date.now();
		const current = getActiveConversation();

		popupWorkspace.tables.conversations.set({
			id,
			title: opts?.title ?? 'New Chat',
			parentId: opts?.parentId,
			sourceMessageId: opts?.sourceMessageId,
			systemPrompt: opts?.systemPrompt,
			provider: current?.provider ?? DEFAULT_PROVIDER,
			model: current?.model ?? DEFAULT_MODEL,
			createdAt: now,
			updatedAt: now,
			_v: 1,
		});

		switchConversation(id);
		return id;
	}

	/**
	 * Switch to a different conversation.
	 *
	 * Changes which conversation the getters read from. If the previous
	 * conversation was streaming, it continues in the background.
	 *
	 * For idle instances (not currently streaming), refreshes messages
	 * from Y.Doc to ensure the view reflects the latest persisted state
	 * (e.g., a response that completed in the background since the user
	 * last viewed this conversation).
	 */
	function switchConversation(conversationId: string) {
		activeConversationId = conversationId;

		// Refresh idle instances from Y.Doc so the view is always current.
		// Streaming instances keep their internal state (includes the
		// in-progress assistant message that Y.Doc doesn't have yet).
		const instance = chatInstances.get(conversationId);
		if (instance && !instance.isLoading) {
			instance.setMessages(loadMessagesForConversation(conversationId));
		}
	}

	/**
	 * Delete a conversation and all its messages.
	 *
	 * Uses a Y.Doc batch so the observer fires once (not N+1 times).
	 * Stops any active stream for the conversation and removes its
	 * ChatClient instance. If the deleted conversation was active,
	 * switches to the most recent remaining one.
	 */
	function deleteConversation(conversationId: string) {
		// Stop and discard the ChatClient instance
		const instance = chatInstances.get(conversationId);
		if (instance) {
			instance.stop();
			chatInstances.delete(conversationId);
		}

		const messages = popupWorkspace.tables.chatMessages
			.getAllValid()
			.filter((m) => m.conversationId === conversationId);

		popupWorkspace.batch(() => {
			for (const m of messages) {
				popupWorkspace.tables.chatMessages.delete(m.id);
			}
			popupWorkspace.tables.conversations.delete(conversationId);
		});

		// Switch away if we deleted the active conversation
		if (activeConversationId === conversationId) {
			const remaining = popupWorkspace.tables.conversations
				.getAllValid()
				.sort((a, b) => b.updatedAt - a.updatedAt);

			const first = remaining[0];
			if (first) {
				switchConversation(first.id);
			} else {
				activeConversationId = null;
			}
		}
	}

	/**
	 * Rename a conversation.
	 *
	 * Writes to Y.Doc — the observer propagates the change to the
	 * reactive `conversations` list and `activeConversation` derived.
	 */
	function renameConversation(conversationId: string, title: string) {
		const conv = conversations.find((c) => c.id === conversationId);
		if (!conv) return;

		popupWorkspace.tables.conversations.set({
			...conv,
			title,
			updatedAt: Date.now(),
		});
	}

	// ── Provider / Model (per-conversation) ───────────────────────────

	/**
	 * Update the active conversation's provider.
	 *
	 * Auto-selects the first model for the new provider so the user
	 * always has a valid model selected after switching providers.
	 */
	function setProvider(providerName: string) {
		const conv = getActiveConversation();
		if (!conv) return;

		const models = PROVIDER_MODELS[providerName as Provider];
		popupWorkspace.tables.conversations.set({
			...conv,
			provider: providerName,
			model: models?.[0] ?? conv.model,
			updatedAt: Date.now(),
		});
	}

	/** Update the active conversation's model. */
	function setModel(modelName: string) {
		const conv = getActiveConversation();
		if (!conv) return;

		popupWorkspace.tables.conversations.set({
			...conv,
			model: modelName,
			updatedAt: Date.now(),
		});
	}

	// ── Public API ────────────────────────────────────────────────────

	return {
		// ── Chat State (reactive via TanStack AI runes) ───────────────

		/**
		 * The current conversation's messages (reactive).
		 *
		 * Reads from the active conversation's ChatClient instance.
		 * When no conversation is active, returns an empty array.
		 */
		get messages() {
			if (!activeConversationId) return [];
			return ensureChat(activeConversationId).messages;
		},

		/**
		 * Whether a response is currently streaming for the active conversation.
		 *
		 * Only reflects the active conversation's state — other conversations
		 * may be streaming in the background without affecting this.
		 */
		get isLoading() {
			if (!activeConversationId) return false;
			return ensureChat(activeConversationId).isLoading;
		},

		/**
		 * The latest error from the active conversation's stream, if any.
		 *
		 * Scoped to the active conversation — background stream errors
		 * don't leak into the current view.
		 */
		get error() {
			if (!activeConversationId) return null;
			return ensureChat(activeConversationId).error;
		},

		/**
		 * Fine-grained connection status for the active conversation.
		 *
		 * More granular than `isLoading` — distinguishes between idle,
		 * streaming, and other states. Useful for nuanced UI indicators
		 * (e.g., "connecting..." vs "generating...").
		 */
		get status() {
			if (!activeConversationId) return 'ready' as const;
			return ensureChat(activeConversationId).status;
		},

		// ── Conversation Management ───────────────────────────────────

		/** All conversations, sorted by most recently updated first (reactive). */
		get conversations() {
			return conversations;
		},

		/** The active conversation's ID, or null if none. */
		get activeConversationId() {
			return activeConversationId;
		},

		/** The active conversation's full metadata, or null if none (reactive). */
		get activeConversation() {
			return activeConversation;
		},

		createConversation,
		switchConversation,
		deleteConversation,
		renameConversation,

		// ── Provider / Model (per-conversation) ───────────────────────

		/** Current provider name (reads from active conversation). */
		get provider() {
			return activeConversation?.provider ?? DEFAULT_PROVIDER;
		},
		set provider(value: string) {
			setProvider(value);
		},

		/** Current model name (reads from active conversation). */
		get model() {
			return activeConversation?.model ?? DEFAULT_MODEL;
		},
		set model(value: string) {
			setModel(value);
		},

		/** List of available provider names. */
		get availableProviders() {
			return AVAILABLE_PROVIDERS;
		},

		/**
		 * Get the curated model list for a given provider.
		 *
		 * @example
		 * ```typescript
		 * aiChatState.modelsForProvider('openai')
		 * // → ['gpt-4o', 'gpt-4o-mini', 'o3-mini']
		 * ```
		 */
		modelsForProvider(providerName: string): readonly string[] {
			return PROVIDER_MODELS[providerName as Provider] ?? [];
		},

		// ── Chat Actions ──────────────────────────────────────────────

		/**
		 * Send a user message and begin streaming the assistant response.
		 *
		 * If no conversation is active, one is auto-created with the
		 * message text as its title (truncated to 50 characters).
		 *
		 * Writes the user message to Y.Doc before sending, and persists
		 * the assistant response via `onFinish`.
		 */
		sendMessage(content: string) {
			if (!content.trim()) return;

			// Auto-create conversation if none active
			let convId = activeConversationId;
			if (!convId) {
				convId = createConversation({
					title: content.trim().slice(0, 50),
				});
			}

			const userMessageId = generateId();

			// Write user message to Y.Doc
			popupWorkspace.tables.chatMessages.set({
				id: userMessageId,
				conversationId: convId,
				role: 'user',
				parts: [{ type: 'text', content }],
				createdAt: Date.now(),
				_v: 1,
			});

			// Touch updatedAt so this conversation floats to top
			const conv = getActiveConversation();
			if (conv) {
				popupWorkspace.tables.conversations.set({
					...conv,
					updatedAt: Date.now(),
				});
			}

			// Send via this conversation's ChatClient (triggers SSE streaming)
			void ensureChat(convId).sendMessage({ content, id: userMessageId });
		},

		/**
		 * Regenerate the last assistant message.
		 *
		 * Deletes the old assistant message from Y.Doc, then calls
		 * `reload()` which re-requests a response from the server.
		 * The new response is persisted via `onFinish`.
		 */
		reload() {
			if (!activeConversationId) return;

			const chat = ensureChat(activeConversationId);
			const lastMessage = chat.messages.at(-1);
			if (lastMessage?.role === 'assistant') {
				popupWorkspace.tables.chatMessages.delete({ id: lastMessage.id });
			}
			void chat.reload();
		},

		/** Stop the active conversation's streaming response. */
		stop() {
			if (!activeConversationId) return;
			ensureChat(activeConversationId).stop();
		},

		/**
		 * Whether a specific conversation is currently streaming a response.
		 *
		 * Useful for showing background streaming indicators in the
		 * conversation list — e.g., a pulsing dot next to conversations
		 * that are generating responses while the user views another.
		 *
		 * Returns `false` for conversations that don't have a ChatClient
		 * instance (never opened or evicted from cache).
		 *
		 * @example
		 * ```svelte
		 * {#if aiChatState.isStreaming(conv.id)}
		 *   <span class="animate-pulse rounded-full bg-primary size-1.5" />
		 * {/if}
		 * ```
		 */
		isStreaming(conversationId: string): boolean {
			return chatInstances.get(conversationId)?.isLoading ?? false;
		},
	};
}

export const aiChatState = createAiChatState();

// ─────────────────────────────────────────────────────────────────────────────
// UIMessage Boundary (co-located from ui-message.ts)
// ─────────────────────────────────────────────────────────────────────────────

/**
 * Compile-time drift detection for TanStack AI message types.
 *
 * The workspace schema stores message parts as `unknown[]` because:
 * 1. Parts are always produced by TanStack AI — never user-constructed
 * 2. Runtime validation of guaranteed-correct data wastes CPU
 * 3. Replicating 8 complex part types in arktype is fragile to upgrades
 *
 * Instead, we use compile-time assertions to catch drift when upgrading
 * TanStack AI. If the MessagePart shape changes, these assertions fail
 * and the build breaks — forcing us to update our understanding.
 *
 * @see https://tanstack.com/ai/latest — UIMessage / MessagePart types
 * @see https://www.totaltypescript.com/how-to-test-your-types#rolling-your-own — Expect / Equal
 */

// ── Type test utilities ───────────────────────────────────────────────
// Rolling-your-own type testing from Total TypeScript.
// @see https://www.totaltypescript.com/how-to-test-your-types#rolling-your-own

type Expect<T extends true> = T;
type Equal<X, Y> =
	(<T>() => T extends X ? 1 : 2) extends <T>() => T extends Y ? 1 : 2
		? true
		: false;

// ── Derive the actual MessagePart type from UIMessage ─────────────────
// This is the type that gets stored in Y.Doc via onFinish/sendMessage.

type TanStackMessagePart = UIMessage['parts'][number];

// ── Compile-time drift detection ──────────────────────────────────────
// If TanStack AI adds, removes, or renames a part type, TypeScript
// reports a type error here — forcing us to update our understanding.

type ExpectedPartTypes =
	| 'text'
	| 'image'
	| 'audio'
	| 'video'
	| 'document'
	| 'tool-call'
	| 'tool-result'
	| 'thinking';

type _DriftCheck = Expect<
	Equal<TanStackMessagePart['type'], ExpectedPartTypes>
>;

// ── Typed boundary: unknown[] → MessagePart[] ─────────────────────────

/**
 * Convert a persisted chat message to a TanStack AI UIMessage.
 *
 * This is the single boundary where `unknown[]` is cast to `MessagePart[]`.
 * Safe because parts are always produced by TanStack AI and round-tripped
 * through Y.Doc serialization (structuredClone-compatible, lossless for
 * plain objects).
 */
function toUiMessage(message: ChatMessage): UIMessage {
	return {
		id: message.id,
		role: message.role,
		parts: message.parts as TanStackMessagePart[],
		createdAt: new Date(message.createdAt),
	};
}
