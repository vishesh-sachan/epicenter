# AI Plugin for Epicenter Server

**Date**: 2026-02-20
**Status**: Draft
**Author**: AI-assisted

## Overview

Add a `createAIPlugin` to `@epicenter/server` that exposes a streaming AI chat endpoint via TanStack AI. Users bring their own API keys (stored locally in settings), send them per-request, and the server proxies to LLM providers via SSE. The Svelte client consumes the stream via `@tanstack/ai-svelte`'s `useChat()`.

## Motivation

### Current State

Whispering handles AI completions directly in the Tauri app — each provider SDK is imported and called client-side:

```typescript
// apps/whispering/src/lib/services/isomorphic/completion/anthropic.ts
async complete({ apiKey, model, systemPrompt, userPrompt }) {
  const client = new Anthropic({ apiKey });
  const message = await client.messages.create({ model, messages: [...] });
  return Ok(message.content[0].text);
}
```

```typescript
// apps/whispering/src/lib/query/isomorphic/transformer.ts
case 'Anthropic':
  return services.completions.anthropic.complete({
    apiKey: settings.value['apiKeys.anthropic'],
    model: step['prompt_transform.inference.provider.Anthropic.model'],
    systemPrompt,
    userPrompt,
  });
```

This creates problems:

1. **No streaming chat**: Current completion services return full responses, not streaming SSE. Building a chat assistant requires real-time streaming.
2. **No server-side AI**: The server (`packages/server`) has sync and workspace plugins but no AI capabilities. Other apps in the ecosystem (Epicenter Assistant) need a server-side chat endpoint.
3. **Duplicated provider wiring**: Each new app would need to re-implement provider adapter selection, API key handling, and streaming mechanics.

### Desired State

A composable Elysia plugin that any Epicenter app can mount:

```typescript
import { createServer } from '@epicenter/server';
import { createAIPlugin } from '@epicenter/server/ai';

const server = createServer(blogClient, { port: 3913 });
server.app.use(new Elysia({ prefix: '/ai' }).use(createAIPlugin()));
server.start();

// POST /ai/chat → SSE stream
```

Client-side:

```typescript
import { useChat } from '@tanstack/ai-svelte';

const chat = useChat({
	api: 'http://localhost:3913/ai/chat',
	// API key sent via headers, configured in connection adapter
});
```

## Research Findings

### TanStack AI Server Integration

TanStack AI's server-side is framework-agnostic. The core flow is:

```
chat(options) → AsyncIterable<StreamChunk> → toServerSentEventsResponse() → Response
```

`toServerSentEventsResponse()` returns a standard Web `Response` object with SSE headers. Elysia handlers can return `Response` objects directly, making the integration trivial at the mechanics level.

| Framework      | Route Definition               | Response Helper                |
| -------------- | ------------------------------ | ------------------------------ |
| Next.js        | `export async function POST()` | `toServerSentEventsResponse()` |
| TanStack Start | `createFileRoute('/api/chat')` | `toServerSentEventsResponse()` |
| SvelteKit      | `export async function POST()` | `toServerSentEventsResponse()` |
| **Elysia**     | `.post('/chat', handler)`      | `toServerSentEventsResponse()` |

**Key finding**: No Elysia-specific adapter exists in TanStack AI. None is needed — just return the `Response`.

### TanStack AI Svelte Integration

`@tanstack/ai-svelte` provides a `useChat()` binding using Svelte 5 runes. It wraps `@tanstack/ai-client`'s `ChatClient` with `$state`-based reactivity.

| Feature           | Support                             |
| ----------------- | ----------------------------------- |
| `useChat()` rune  | Yes                                 |
| SSE streaming     | Yes                                 |
| HTTP streaming    | Yes                                 |
| Client-side tools | Yes                                 |
| UI components     | No (manual rendering with `marked`) |
| Devtools          | No                                  |

**Key finding**: Svelte integration exists but is thinner than React/Solid/Vue — no UI component library, no devtools. We'd render messages manually (which is fine for Epicenter's design).

### Provider Adapters Available

| Provider   | TanStack AI Adapter      | Currently in Whispering |
| ---------- | ------------------------ | ----------------------- |
| OpenAI     | `@tanstack/ai-openai`    | `openai` SDK            |
| Anthropic  | `@tanstack/ai-anthropic` | `@anthropic-ai/sdk`     |
| Google     | `@tanstack/ai-gemini`    | `@google/generative-ai` |
| Groq       | `@tanstack/ai-grok`      | `groq-sdk`              |
| Ollama     | `@tanstack/ai-ollama`    | Not used                |
| Mistral    | Not available            | `@mistralai/mistralai`  |
| ElevenLabs | Not available            | `elevenlabs`            |

**Implication**: TanStack AI covers the major chat providers. Transcription-specific providers (ElevenLabs, Mistral for transcription) stay on raw SDKs in Whispering.

### Existing Server Plugin Pattern

The server uses a consistent plugin composition pattern:

```typescript
// packages/server/src/server.ts
const app = new Elysia()
  .use(openapi({ ... }))
  .use(new Elysia({ prefix: '/rooms' }).use(createSyncPlugin({ ... })))
  .use(new Elysia({ prefix: '/workspaces' }).use(createWorkspacePlugin(clients)))
  .get('/', () => ({ name: 'Epicenter API', ... }));
```

Each plugin:

- Is a function returning `new Elysia()` with routes
- Takes a config object
- Is prefix-agnostic (caller mounts with `new Elysia({ prefix })`)
- Uses Elysia's `t` for request/response schemas

## Design Decisions

| Decision                | Choice                                    | Rationale                                                         |
| ----------------------- | ----------------------------------------- | ----------------------------------------------------------------- |
| Plugin vs raw routes    | Elysia plugin (`createAIPlugin`)          | Matches existing composition pattern; gets auth, OpenAPI for free |
| Route mount point       | `/ai/chat`                                | Cross-cutting, not workspace-scoped yet; simple starting point    |
| API key transport       | `x-provider-api-key` header               | Avoids conflating with server auth; explicit about what it is     |
| Provider selection      | Client sends `provider` + `model` in body | Server validates against whitelist; keeps control server-side     |
| TanStack AI vs raw SDKs | TanStack AI for chat only                 | Unified streaming + SSE; don't replace working transcription code |
| Streaming protocol      | SSE via `toServerSentEventsResponse()`    | TanStack AI's default; `useChat()` expects it                     |
| API key storage         | Never server-side                         | Local-first principle; keys stay in client settings               |
| "Spells" abstraction    | Deferred                                  | Build the endpoint first; abstraction patterns will emerge later  |

## Architecture

### System Overview

```
┌─────────────────────────────────────────────────────────────────────┐
│                        Epicenter Server                             │
│                                                                     │
│  ┌──────────────┐  ┌──────────────────┐  ┌───────────────────────┐ │
│  │  Sync Plugin  │  │ Workspace Plugin │  │     AI Plugin         │ │
│  │  /rooms/      │  │ /workspaces/     │  │     /ai/              │ │
│  │               │  │                  │  │                       │ │
│  │  WS y-doc     │  │  REST CRUD for   │  │  POST /chat           │ │
│  │  sync         │  │  tables/actions  │  │  → SSE stream         │ │
│  └──────────────┘  └──────────────────┘  └───────────────────────┘ │
│                                                                     │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  OpenAPI (Scalar UI)                                         │   │
│  └─────────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────┘
```

### Request Flow

```
Client (Tauri App / Browser)                  Server (Elysia)
┌────────────────────────────┐    POST /ai/chat    ┌───────────────────────┐
│                            │    ─────────────▶   │                       │
│  useChat() sends:          │    Headers:          │  1. Extract API key   │
│  {                         │    x-provider-api-key│     from header       │
│    messages: [...],        │    ───────────────   │                       │
│    provider: "openai",     │    Body:             │  2. Validate provider │
│    model: "gpt-4o"         │    { messages, ...}  │     against whitelist │
│  }                         │                      │                       │
│                            │                      │  3. Create adapter:   │
│  Settings store            │                      │     openaiText({      │
│  ┌──────────────────┐      │                      │       apiKey,         │
│  │ apiKeys.openai:  │──────│──▶ (sent in header)  │       model           │
│  │ "sk-..."         │      │                      │     })                │
│  └──────────────────┘      │                      │                       │
│                            │    SSE stream         │  4. chat({            │
│  useChat() receives:       │    ◀─────────────    │       adapter,        │
│  streaming chunks          │    text/event-stream  │       messages        │
│  → updates $state          │                      │     })                │
│  → reactive UI             │                      │                       │
│                            │                      │  5. toSSEResponse()   │
└────────────────────────────┘                      └───────────────────────┘
                                                              │
                                                              ▼
                                                    ┌───────────────────┐
                                                    │   LLM Provider    │
                                                    │   (OpenAI, etc.)  │
                                                    │                   │
                                                    │   API key passed  │
                                                    │   directly from   │
                                                    │   client → here   │
                                                    └───────────────────┘
```

### Plugin Composition (createServer)

```
createServer(clients, options)
│
├── openapi()                           ── /openapi, /openapi/json
│
├── new Elysia({ prefix: '/rooms' })
│   └── createSyncPlugin(config)        ── WS /:room/sync, GET/POST /:room/doc
│
├── new Elysia({ prefix: '/workspaces' })
│   └── createWorkspacePlugin(clients)  ── REST /:id/tables/:table, /:id/actions/:action
│
├── new Elysia({ prefix: '/ai' })       ── NEW
│   └── createAIPlugin(config)          ── POST /chat
│
└── GET /                               ── Discovery endpoint
```

### API Key Flow — What NOT To Do

```
                    NEVER DO THIS
                    ─────────────
 ┌──────────┐   sync    ┌──────────┐
 │ Client A │ ────────▶ │  Y.Doc   │  ← API keys in synced doc = leaked to all clients
 └──────────┘           └──────────┘
                            │ sync
 ┌──────────┐              ▼
 │ Client B │ ◀──────── (receives Client A's keys!)
 └──────────┘

                    DO THIS INSTEAD
                    ───────────────
 ┌──────────┐   per-request header   ┌──────────┐   API call   ┌──────────┐
 │ Client   │ ─────────────────────▶ │  Server  │ ───────────▶ │ Provider │
 │ (keys in │   x-provider-api-key   │ (no key  │  (key used   │ (OpenAI) │
 │  local   │   Authorization:...    │  storage)│   once, then │          │
 │  store)  │                        │          │   discarded) │          │
 └──────────┘                        └──────────┘              └──────────┘
```

## Implementation Plan

### Phase 1: Server-Side AI Plugin

New files in `packages/server/src/ai/`.

- [x] **1.1** Create `packages/server/src/ai/plugin.ts` — `createAIPlugin(config?)` Elysia plugin
  - Registers `POST /chat` route
  - Request schema: `{ messages: ModelMessage[], provider: string, model: string, conversationId?: string, systemPrompt?: string }`
  - Extracts API key from `x-provider-api-key` header
  - Creates provider adapter based on `provider` + `model` + `apiKey`
  - Creates `AbortController` for client disconnect cleanup
  - Calls `chat({ adapter, messages, abortController, systemPrompts })` → `toServerSentEventsResponse(stream, { abortController })`
  - Wraps in try/catch for provider errors (bad key, rate limit) → returns structured 502 error
  - Returns the `Response` directly on success

- [x] **1.2** Create `packages/server/src/ai/adapters.ts` — Provider adapter factory
  - `createAdapter(provider, model, apiKey)` → returns TanStack AI adapter
  - Whitelist of supported providers: `openai`, `anthropic`, `gemini`, `ollama`, `grok`
  - Type-safe: provider string maps to correct adapter factory
  - Ollama doesn't need an API key (local)

- [x] **1.3** Create `packages/server/src/ai/index.ts` — Public exports
  - Export `createAIPlugin` and `AIPluginConfig` type

- [x] **1.4** Add sub-entry to `packages/server/package.json`
  - `"./ai": "./src/ai/index.ts"` (matches existing `./sync`, `./workspace` pattern)

- [x] **1.5** Wire into `createServer` in `packages/server/src/server.ts`
  - Mount as `new Elysia({ prefix: '/ai' }).use(createAIPlugin())`

- [x] **1.6** Add TanStack AI dependencies to `packages/server/package.json`
  - `@tanstack/ai` (core)
  - `@tanstack/ai-openai`, `@tanstack/ai-anthropic`, `@tanstack/ai-gemini`, `@tanstack/ai-ollama`, `@tanstack/ai-grok` (adapters)

- [ ] **1.7** Update `packages/server/README.md`
  - Document the new `/ai/chat` endpoint, request/response format, and API key handling

- [x] **1.8** Add basic test in `packages/server/src/ai/plugin.test.ts`
  - Test route registration, request validation, error responses
  - Tests cover: 401 missing key, 400 unsupported provider, 422 missing body, ollama key exemption, config.providers restriction, prefix mount

### Phase 2: Client-Side Integration

Integration in the Svelte app(s) that consume the AI chat endpoint.

- [ ] **2.1** Add `@tanstack/ai-svelte` and `@tanstack/ai-client` to the consuming app's dependencies

- [ ] **2.2** Create a chat service/query layer that uses `useChat()` from `@tanstack/ai-svelte`
  - Configure with server URL (`http://localhost:3913/ai/chat`)
  - Custom connection adapter that injects `x-provider-api-key` header from settings
  - Handle provider/model selection from user settings

- [ ] **2.3** Build chat UI components
  - Message list (streaming text display)
  - Input form
  - Provider/model selector
  - Loading/error states

- [ ] **2.4** Wire API key management
  - Reuse existing settings pattern (`apiKeys.openai`, `apiKeys.anthropic`, etc.)
  - Selected provider's key is sent in header per-request
  - Show "API key required" prompt if key is missing for selected provider

## Conceptual Code

### createAIPlugin (Phase 1)

```typescript
// packages/server/src/ai/plugin.ts
import { Elysia, t } from 'elysia';
import { chat, toServerSentEventsResponse } from '@tanstack/ai';
import { createAdapter } from './adapters';

export type AIPluginConfig = {
	/** Optional: override supported providers. Defaults to all built-in adapters. */
	providers?: string[];
};

export function createAIPlugin(config?: AIPluginConfig) {
	return new Elysia().post(
		'/chat',
		async ({ body, headers, set }) => {
			const apiKey = headers['x-provider-api-key'];
			const { messages, provider, model, conversationId, systemPrompt } = body;

			// Ollama is local — no key needed
			if (provider !== 'ollama' && !apiKey) {
				set.status = 401;
				return { error: 'Missing x-provider-api-key header' };
			}

			const adapter = createAdapter(provider, model, apiKey);
			if (!adapter) {
				set.status = 400;
				return { error: `Unsupported provider: ${provider}` };
			}

			// AbortController for cleanup when client disconnects mid-stream.
			// TanStack AI best practice: always pass abortController to chat() and
			// toServerSentEventsResponse() so both the LLM API call and the SSE
			// stream are cancelled on disconnect.
			const abortController = new AbortController();

			try {
				const stream = chat({
					adapter,
					messages,
					conversationId,
					abortController,
					// chat() has a first-class systemPrompts parameter — use it
					// instead of prepending a system message to the messages array.
					...(systemPrompt ? { systemPrompts: [systemPrompt] } : {}),
				});

				return toServerSentEventsResponse(stream, { abortController });
			} catch (error) {
				// Provider errors (bad API key, rate limit, model not found) throw
				// synchronously before streaming starts. Catch and return structured
				// error responses instead of letting Elysia's default 500 handle it.
				const message =
					error instanceof Error ? error.message : 'Unknown error';
				set.status = 502;
				return { error: `Provider error: ${message}` };
			}
		},
		{
			body: t.Object({
				messages: t.Array(t.Any()), // ModelMessage[] — validated by TanStack AI
				provider: t.String(),
				model: t.String(),
				conversationId: t.Optional(t.String()),
				systemPrompt: t.Optional(t.String()),
			}),
		},
	);
}
```

### createAdapter (Phase 1)

> **CORRECTED**: The original conceptual code used `openaiText({ apiKey, model })` which is the
> env-auto-detect variant. The actual API uses positional arguments: `createOpenaiChat(model, apiKey)`.
> See "Phase 1 Review" section below for details.

```typescript
// packages/server/src/ai/adapters.ts
import { createOpenaiChat } from '@tanstack/ai-openai';
import { createAnthropicChat } from '@tanstack/ai-anthropic';
import { createGeminiChat } from '@tanstack/ai-gemini';
import { createOllamaChat } from '@tanstack/ai-ollama';
import { createGrokText } from '@tanstack/ai-grok';

const ADAPTER_FACTORIES = {
	openai: (model, apiKey) => createOpenaiChat(model, apiKey),
	anthropic: (model, apiKey) => createAnthropicChat(model, apiKey),
	gemini: (model, apiKey) => createGeminiChat(model, apiKey),
	ollama: (model, _apiKey) => createOllamaChat(model),
	grok: (model, apiKey) => createGrokText(model, apiKey),
};

export function createAdapter(
	provider: string,
	model: string,
	apiKey: string = '',
) {
	const factory = ADAPTER_FACTORIES[provider];
	if (!factory) return undefined;
	return factory(model, apiKey);
}
```

### Server Composition (Phase 1)

```typescript
// packages/server/src/server.ts (updated)
import { createAIPlugin } from './ai';

const app = new Elysia()
  .use(openapi({ ... }))
  .use(new Elysia({ prefix: '/rooms' }).use(createSyncPlugin({ ... })))
  .use(new Elysia({ prefix: '/workspaces' }).use(createWorkspacePlugin(clients)))
  .use(new Elysia({ prefix: '/ai' }).use(createAIPlugin()))   // ← NEW
  .get('/', () => ({
    name: 'Epicenter API',
    version: '1.0.0',
    workspaces: Object.keys(workspaces),
    actions: allActionPaths,
  }));
```

### Client Usage (Phase 2)

```svelte
<script lang="ts">
	import { useChat } from '@tanstack/ai-svelte';
	import { sseAdapter } from '@tanstack/ai-client';
	import { settings } from '$lib/state/settings.svelte';

	const chat = useChat({
		api: 'http://localhost:3913/ai/chat',
		connectionAdapter: sseAdapter({
			headers: () => ({
				'x-provider-api-key':
					settings.value[`apiKeys.${settings.value['ai.provider']}`],
			}),
			body: () => ({
				provider: settings.value['ai.provider'],
				model: settings.value['ai.model'],
			}),
		}),
	});
</script>

{#each chat.messages as message}
	<div class={message.role === 'user' ? 'text-right' : 'text-left'}>
		{message.content}
	</div>
{/each}

<form onsubmit={chat.handleSubmit}>
	<input bind:value={chat.input} placeholder="Ask anything..." />
</form>
```

## Edge Cases

### Missing or Invalid API Key

1. Client sends request without `x-provider-api-key` header
2. Server returns `401 { error: "Missing x-provider-api-key header" }`
3. Client shows "Add your API key in Settings" prompt

### Unsupported Provider

1. Client sends `{ provider: "mistral" }` (not in adapter whitelist)
2. Server returns `400 { error: "Unsupported provider: mistral" }`
3. Client should only show providers from the supported list

### Client Disconnects Mid-Stream

1. User navigates away or closes tab during streaming
2. The `abortController` passed to both `chat()` and `toServerSentEventsResponse()` signals cancellation
3. This aborts the in-flight LLM API call (saving cost) and tears down the SSE stream cleanly
4. The conceptual code already wires this — see `createAIPlugin` above

### Ollama (Local Provider)

1. User selects Ollama — no API key needed
2. Server skips key validation for `provider: "ollama"`
3. Ollama must be running locally; connection failure returns a clear error

### API Key in Synced Workspace Data

1. Developer accidentally stores API key in a Yjs-synced table
2. Key replicates to all connected clients
3. **Mitigation**: API keys MUST stay in local-only storage (IndexedDB settings). The AI plugin never persists keys. Document this clearly.

## DeepWiki Research Findings

Verified against DeepWiki analysis of [TanStack/ai](https://deepwiki.com/TanStack/ai) (indexed at commit b4d988) and [elysiajs/elysia](https://deepwiki.com/elysiajs/elysia) (indexed at commit 3edbde).

### TanStack AI API Surface (Verified)

- **`chat()`** is the correct function for conversation-oriented streaming. There is also `generate()` for simpler single-turn prompts, but `chat()` is what we need for a chat endpoint.
- **`toServerSentEventsResponse(stream, init?)`** returns a standard Web `Response` with headers `Content-Type: text/event-stream`, `Cache-Control: no-cache`, `Connection: keep-alive`. Elysia handlers can return `Response` directly.
- **`toServerSentEventsStream(stream, abortController?)`** is the lower-level variant returning `ReadableStream<Uint8Array>` — not needed here.
- **SSE format**: Each chunk is `data: {JSON}\n\n`, stream terminates with `data: [DONE]\n\n`.
- **`abortController`** is accepted by both `chat()` and `toServerSentEventsResponse()` for cancellation propagation.
- **`systemPrompts`** is a first-class `chat()` parameter — no need to prepend system messages to the messages array.
- **`chat()` returns `AsyncIterable<StreamChunk>`** where `StreamChunk` is a discriminated union: `content`, `thinking`, `tool_call`, `tool_result`, `tool-input-available`, `approval-requested`, `done`, `error`.

### Adapter Package Exports (Verified — then Corrected)

Each adapter package exports from its root `.` path only. However, each package exports **two** factory functions — one env-auto-detect variant and one explicit-key variant:

| Provider  | Env-auto-detect (reads env var) | Explicit key (our use case)          |
| --------- | ------------------------------- | ------------------------------------ |
| OpenAI    | `openaiText(model, config?)`    | `createOpenaiChat(model, apiKey)`    |
| Anthropic | `anthropicText(model, config?)` | `createAnthropicChat(model, apiKey)` |
| Gemini    | `geminiText(model, config?)`    | `createGeminiChat(model, apiKey)`    |
| Ollama    | `ollamaText(model)`             | `createOllamaChat(model, host?)`     |
| Grok      | `grokText(model, config?)`      | `createGrokText(model, apiKey)`      |

**Critical finding**: The original conceptual code used `openaiText({ apiKey, model })` which is **wrong** in two ways:

1. `openaiText` reads API keys from `process.env`, not from arguments — no `apiKey` param
2. The signature is `openaiText(model)` (positional string), not `openaiText({ model })`

Since our server receives API keys per-request from clients (not from env vars), we must use the explicit-key variants (`createOpenaiChat`, `createAnthropicChat`, etc.).

```typescript
// ✅ Correct — explicit API key variants for per-request keys
import { createOpenaiChat } from '@tanstack/ai-openai';
import { createAnthropicChat } from '@tanstack/ai-anthropic';
import { createGeminiChat } from '@tanstack/ai-gemini';
import { createOllamaChat } from '@tanstack/ai-ollama';
import { createGrokText } from '@tanstack/ai-grok';

// ❌ Wrong — these read from process.env, not per-request
// import { openaiText } from '@tanstack/ai-openai';
```

Current versions: `@tanstack/ai@0.5.1`, `@tanstack/ai-openai@0.5.0`. Each adapter has a peer dependency on `@tanstack/ai` at `workspace:^`.

### Svelte Integration (Verified)

- `@tanstack/ai-svelte` requires Svelte 5.0.0+ (runes-based).
- Provides `useChat()` binding wrapping `@tanstack/ai-client`'s `ChatClient` with `$state` reactivity.
- **No UI component library** exists for Svelte yet (unlike React, Solid, Vue) — messages must be rendered manually.
- **No devtools** package for Svelte yet.
- Connection adapters (`fetchServerSentEvents`, `fetchHttpStream`) are imported from `@tanstack/ai-client`.

### Elysia Plugin Mechanics (Verified)

- Elysia plugins are `new Elysia()` instances registered via `.use()`.
- Handlers returning `Response` objects bypass Elysia's serialization — SSE streaming works natively.
- Three-tier scope system (global/scoped/local) — our plugin uses local scope correctly.
- TypeBox `t.*` schemas provide runtime validation and OpenAPI generation.

## Open Questions

1. **Should the AI plugin be included in `createServer` by default, or opt-in?**
   - Options: (a) Always mounted, (b) Opt-in via config flag, (c) Separate import
   - **Recommendation**: Opt-in. `createServer(clients, { ai: true })` or manual `.use()`. Not everyone running a sync server wants AI routes. Defer until usage patterns emerge.

2. **System prompts — where do they come from?**
   - Options: (a) Client sends in body, (b) Server-side config, (c) Workspace-stored "spells"
   - **Decision**: Client sends optional `systemPrompt` string in body. The plugin passes it to `chat()` via its first-class `systemPrompts` parameter. Server-side presets can come later.

3. **Tools / function calling — Phase 1 or later?**
   - Options: (a) Support tools in Phase 1, (b) Defer to Phase 3
   - **Recommendation**: Defer. Get basic chat streaming working first. Tools add significant complexity (server-side execution, client-side tools, approval flows).

4. **Rate limiting / cost guardrails?**
   - Since users bring their own keys, rate limiting is the provider's concern
   - **Recommendation**: Don't implement rate limiting initially. Add optional `maxTokens` cap later if needed.

5. **Conversation persistence?**
   - Options: (a) Stateless (client manages history), (b) Server stores in workspace table
   - **Recommendation**: Stateless. Client sends full message history each request. This is how TanStack AI works by default and avoids server-side state.

## Success Criteria

- [ ] `POST /ai/chat` returns SSE stream for a valid request with API key
- [ ] Provider adapter selection works for openai, anthropic, gemini, ollama, grok
- [ ] Missing API key returns 401; unsupported provider returns 400
- [ ] Client disconnect aborts the server-side stream
- [ ] API keys are never logged, persisted, or synced by the server
- [ ] Plugin mounts cleanly alongside existing sync and workspace plugins
- [ ] OpenAPI docs include the `/ai/chat` endpoint
- [ ] `packages/server` typecheck passes
- [ ] Basic test coverage for route validation and error paths

## URL Hierarchy (Updated)

```
/                                              - API root / discovery
/openapi                                       - Scalar UI documentation
/openapi/json                                  - OpenAPI spec (JSON)
/rooms/                                        - Active rooms with connection counts
/rooms/{workspaceId}/sync                      - WebSocket sync (y-websocket protocol)
/rooms/{workspaceId}/doc                       - Document state (GET/POST)
/workspaces/{workspaceId}/tables/{table}       - RESTful table CRUD
/workspaces/{workspaceId}/tables/{table}/{id}  - Single row operations
/workspaces/{workspaceId}/actions/{action}     - Workspace action endpoints
/ai/chat                                       - AI chat (SSE streaming)    ← NEW
```

## References

- `packages/server/src/server.ts` — Main server composition, plugin mounting pattern
- `packages/server/src/sync/plugin.ts` — Reference plugin implementation (sync)
- `packages/server/src/workspace/plugin.ts` — Reference plugin implementation (workspace)
- `packages/server/package.json` — Dependencies, sub-entries pattern
- `apps/whispering/src/lib/settings/settings.ts` — API key storage pattern
- `apps/whispering/src/lib/services/isomorphic/completion/` — Existing completion services
- TanStack AI docs: `chat()` function, `toServerSentEventsResponse()`, adapter APIs
- TanStack AI Svelte: `@tanstack/ai-svelte` `useChat()` binding

## Phase 1 Review

### What Was Implemented

All Phase 1 items (1.1–1.8) except 1.7 (README update, deferred to Phase 2 when the full endpoint behavior is documented with client examples).

**Files created:**

```
packages/server/src/ai/
├── adapters.ts      — Provider adapter factory (5 providers)
├── plugin.ts        — createAIPlugin() Elysia plugin with POST /chat
├── index.ts         — Public exports for @epicenter/server/ai
└── plugin.test.ts   — 6 tests: auth, validation, error paths, prefix mount
```

**Files modified:**

- `packages/server/package.json` — `"./ai"` sub-entry + 6 `@tanstack/ai-*` dependencies
- `packages/server/src/server.ts` — Wired `createAIPlugin()` under `/ai` prefix

### Key Deviation from Spec

The conceptual code in this spec used `openaiText({ apiKey, model })` which does not exist. After reading the actual TanStack AI source code (GitHub `TanStack/ai` at `packages/typescript/ai-*/src/adapters/text.ts`), the correct APIs are:

- `createOpenaiChat(model, apiKey)` — positional `(model: string, apiKey: string)`
- `createAnthropicChat(model, apiKey)` — same pattern
- `createGeminiChat(model, apiKey)` — same pattern
- `createOllamaChat(model, host?)` — no API key, optional host
- `createGrokText(model, apiKey)` — same pattern

The `openaiText(model)` / `anthropicText(model)` variants auto-detect API keys from `process.env.OPENAI_API_KEY` etc., which is wrong for our per-request key model.

### Verification

- `bun run typecheck` — passes clean (0 errors)
- `bun test src/ai/plugin.test.ts` — 6 tests pass (401, 400, 422, ollama exemption, config restriction, prefix mount)
- `bun test src/server.test.ts` — 6 existing tests still pass (no regressions)
