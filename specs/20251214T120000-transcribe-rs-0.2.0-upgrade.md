# transcribe-rs 0.2.0 Upgrade Spec

**Date**: 2025-12-14
**Status**: Completed

## Summary

Upgraded transcribe-rs from 0.1.5 to 0.2.0. The upgrade was straightforward with only one required change: enabling feature flags for the engines we use.

## Changes Made

### Cargo.toml Update

```toml
# Before
transcribe-rs = "0.1.5"

# After
transcribe-rs = { version = "0.2.0", features = ["whisper", "parakeet"] }
```

## Key Findings from Research

### What Changed in 0.2.0

1. **Feature Flags (Breaking Build Change)**: All engines are now optional and must be explicitly enabled via Cargo features
2. **New Engines Added**: Moonshine, Whisperfile (we don't use these)
3. **HTTP Client Migration**: Whisperfile now uses ureq instead of async client
4. **No API Breaking Changes**: The `TranscriptionEngine` trait and all existing engine APIs remain identical

### Available Feature Flags

| Feature | Description | We Use? |
|---------|-------------|---------|
| `whisper` | OpenAI Whisper via whisper-rs | Yes |
| `parakeet` | NVIDIA Parakeet ONNX | Yes |
| `moonshine` | UsefulSensors ONNX | No |
| `whisperfile` | Mozilla whisperfile server | No |
| `openai` | OpenAI API (remote) | No |
| `all` | Enable all engines | No |

### CI Simplification Analysis

**No significant CI simplification is possible** because:

1. **No Pre-built Binaries**: transcribe-rs doesn't provide pre-built binaries
2. **whisper.cpp Compilation Required**: The whisper-rs dependency compiles whisper.cpp from source
3. **GPU Dependencies Required**: Vulkan (Linux/Windows) and Metal (macOS) are required for GPU acceleration

**Minor Optimizations Available**:
- Specifying only needed features (already done) saves ~30-60s per platform
- Using `WHISPER_DONT_GENERATE_BINDINGS=1` could save ~15-30s per platform

### Handy App Patterns

The Handy app (which uses transcribe-rs) follows similar patterns to Whispering:
- Manager pattern for transcription engines
- `Arc<Mutex<Option<Engine>>>` for thread-safe state
- Lazy model loading with idle timeout for unloading
- Uses `ParakeetModelParams::int8()` for quantization

## Migration Notes for Future Reference

If you ever need to update transcribe-rs again:

1. **Feature Flags**: Always specify the engines you need
2. **TranscriptionResult.segments**: In 0.2.0+, segments is `Option<Vec<TranscriptionSegment>>` not `Vec<TranscriptionSegment>` (we don't use segments currently so no code change needed)
3. **No API Changes**: The engine APIs are stable

## Testing

- [x] `cargo check` passes
- [ ] Manual testing with Whisper engine
- [ ] Manual testing with Parakeet engine

## Review

The upgrade was simple. The main change in 0.2.0 is modularizing engines via feature flags, which is actually beneficial as it:
- Reduces compile time by not building unused engines
- Reduces binary size
- Reduces dependencies

Our existing code using `WhisperEngine`, `WhisperInferenceParams`, `ParakeetEngine`, `ParakeetInferenceParams`, `ParakeetModelParams`, and `TimestampGranularity` continues to work unchanged.
